{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des librairies\n",
    "- pixiedust (Kernel Python/Scala)\n",
    "- time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pixiedust\n",
    "import time\n",
    "import os\n",
    "import tweepy\n",
    "import json\n",
    "import json_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration du nom du fichier utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TIME=1521125436.670453\n",
      "env: OUTPUT_TWEET_TXT_FILENAME=output_tweet_1521125436.670453.txt\n",
      "env: TWEET_JSON_FILENAME=input_tweet_1521125436.670453.json\n"
     ]
    }
   ],
   "source": [
    "%env TIME={time.time()}\n",
    "\n",
    "%env OUTPUT_TWEET_TXT_FILENAME=output_tweet_{os.environ['TIME']}.txt\n",
    "%env TWEET_JSON_FILENAME=input_tweet_{os.environ['TIME']}.json\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Collect des données twitter et stockage dans un json 'tweets_time.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_geo=False\n",
    "API_key = json.load(open(\"API_key.txt\", \"r\"))\n",
    "\n",
    "auth = tweepy.OAuthHandler(API_key[\"consumer_key\"], API_key[\"consumer_secret\"])\n",
    "auth.set_access_token(API_key[\"access_token\"], API_key[\"access_token_secret\"])\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "keyword = \"JO\"\n",
    "\n",
    "tweets = api.search( q=keyword,tweet_mode='extended',include_rts=False,count=3200)\n",
    "\n",
    "json_tweets=[]\n",
    "i=0\n",
    "j=0\n",
    "while i<1:\n",
    "\n",
    "\n",
    "    i=i+1\n",
    "    for tweet in tweets:\n",
    "        if with_geo==False or tweet._json[\"place\"]!=None or tweet._json[\"coordinates\"]!=None:\n",
    "            j=j+1\n",
    "            if len(json_tweets)!=0:\n",
    "                b=[x for x in json_tweets if json.loads(x)[\"id\"]==tweet._json[\"id\"]]\n",
    "                if len(b)==0:\n",
    "                    json_tweets.append(json.dumps(tweet._json))\n",
    "            else:\n",
    "                json_tweets.append(json.dumps(tweet._json))\n",
    "\n",
    "json_str = \"\\n\\r\".join(json_tweets)\n",
    "\n",
    "filename = \"tweets-database/\"+os.environ['TWEET_JSON_FILENAME']\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mise à jour de l'algorithme sur le Datacenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from idea.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading global plugins from /Users/Frego/.sbt/1.0/plugins\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from plugins.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading project definition from /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/project\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from build.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mSet current project to SparkSenti (in build file:/Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/)\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mExclude scala-library-2.11.8.jar from the package\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] Creating a distributable package in /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/target/pack\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] Copying libraries to /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/target/pack/lib\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] project jars:\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m/Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/target/scala-2.11/sparksenti_2.11-0.1.jar\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] project dependencies:\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0morg.scala-lang:scala-reflect:2.11.8\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mxalan:xalan:2.7.0\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mcom.typesafe.play:play-functional_2.11:2.6.7\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mjoda-time:joda-time:2.9.9\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mxml-apis:xml-apis:1.3.03\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mcom.fasterxml.jackson.datatype:jackson-datatype-jsr310:2.8.9\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mcom.fasterxml.jackson.core:jackson-core:2.8.9\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mde.jollyday:jollyday:0.4.7\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0medu.stanford.nlp:stanford-corenlp:3.3.0-models\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0medu.stanford.nlp:stanford-corenlp:3.3.0\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mcom.fasterxml.jackson.datatype:jackson-datatype-jdk8:2.8.9\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mcom.typesafe.play:play-json_2.11:2.6.7\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mcom.fasterxml.jackson.core:jackson-annotations:2.8.9\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mxerces:xercesImpl:2.8.0\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0morg.typelevel:macro-compat_2.11:1.1.1\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mcom.fasterxml.jackson.core:jackson-databind:2.8.9\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mcom.googlecode.efficient-java-matrix-library:ejml:0.23\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0morg.scalaz:scalaz-core_2.11:7.2.18\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mcom.io7m.xom:xom:1.2.10\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mjavax.xml.bind:jaxb-api:2.2.7\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] unmanaged dependencies:\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] explicit dependencies:\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] Create a bin folder: /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/target/pack/bin\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] Generating launch scripts\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] main class for test: algo1_worksheet.test\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] Generating /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/target/pack/bin/test\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] Generating /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/target/pack/bin/test.bat\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] packed resource directories = /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/src/pack\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] Generating /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/target/pack/Makefile\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] Generating /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/target/pack/VERSION\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0m[SparkSenti] done.\u001b[0m\n",
      "spawn scp target/scala-2.11/sparksenti_2.11-0.1.jar fregosi1@156.18.90.100:./SparkSenti-0.1/lib\r\n",
      "fregosi1@156.18.90.100's password: \r\n",
      "\r",
      "sparksenti_2.11-0.1.jar                         0%    0     0.0KB/s   --:-- ETA\r",
      "sparksenti_2.11-0.1.jar                       100%   33KB   1.9MB/s   00:00    \r\n",
      "\u001b[0m[\u001b[0m\u001b[32msuccess\u001b[0m] \u001b[0m\u001b[0mTotal time: 8 s, completed 15 mars 2018 15:53:34\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd Algo1\n",
    "sbt \"push\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Envoie des tweets collectés sur le datacenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from idea.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading global plugins from /Users/Frego/.sbt/1.0/plugins\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from plugins.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading project definition from /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/project\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from build.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mSet current project to SparkSenti (in build file:/Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/)\u001b[0m\n",
      "spawn scp input_tweet_1521125436.670453.json fregosi1@156.18.90.100:.\r\n",
      "fregosi1@156.18.90.100's password: \r\n",
      "\r",
      "input_tweet_1521125436.670453.json              0%    0     0.0KB/s   --:-- ETA\r",
      "input_tweet_1521125436.670453.json            100%  504KB   5.0MB/s   00:00    \r\n",
      "spawn ssh fregosi1@156.18.90.100\r\n",
      "fregosi1@156.18.90.100's password: \r\n",
      "Last login: Thu Mar 15 16:02:08 2018 from vpn108-64.vpnusers.ec-lyon.fr\r",
      "\r\n",
      "Welcome to Bright release         7.3\r\n",
      "\r\n",
      "                                                        Based on CentOS Linux 7\r\n",
      "                                                                    ID: #000002\r\n",
      "\r\n",
      "Use the following commands to adjust your environment:\r\n",
      "\r\n",
      "'module avail'            - show available modules\r\n",
      "'module add <module>'     - adds a module to your environment for this session\r\n",
      "'module initadd <module>' - configure module to be loaded at every login\r\n",
      "\r\n",
      "-------------------------------------------------------------------------------\r\n",
      "\u001b]0;fregosi1@bright73:~\u0007[fregosi1@bright73 ~]$ hdfs dfs -put input_tweet_1521125436.670453.json\r\n",
      "\u001b]0;fregosi1@bright73:~\u0007[fregosi1@bright73 ~]$ \n",
      "\u001b[0m[\u001b[0m\u001b[32msuccess\u001b[0m] \u001b[0m\u001b[0mTotal time: 3 s, completed 15 mars 2018 16:04:07\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cp tweets-database/$TWEET_JSON_FILENAME Algo1/$TWEET_JSON_FILENAME\n",
    "cd Algo1\n",
    "sbt \"put $TWEET_JSON_FILENAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution de l'analyse de sentiments des tweets colléctés sur le datacenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from idea.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading global plugins from /Users/Frego/.sbt/1.0/plugins\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from plugins.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading project definition from /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/project\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from build.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mSet current project to SparkSenti (in build file:/Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/)\u001b[0m\n",
      "spawn ssh fregosi1@156.18.90.100\r\n",
      "fregosi1@156.18.90.100's password: \r\n",
      "Last login: Thu Mar 15 16:04:05 2018 from vpn108-64.vpnusers.ec-lyon.fr\r",
      "\r\n",
      "Welcome to Bright release         7.3\r\n",
      "\r\n",
      "                                                        Based on CentOS Linux 7\r\n",
      "                                                                    ID: #000002\r\n",
      "\r\n",
      "Use the following commands to adjust your environment:\r\n",
      "\r\n",
      "'module avail'            - show available modules\r\n",
      "'module add <module>'     - adds a module to your environment for this session\r\n",
      "'module initadd <module>' - configure module to be loaded at every login\r\n",
      "\r\n",
      "-------------------------------------------------------------------------------\r\n",
      "\u001b]0;fregosi1@bright73:~\u0007[fregosi1@bright73 ~]$ echo SparkSenti-0.1/lib/*.jar | tr \" \" \",\"\r\n",
      "SparkSenti-0.1/lib/akka-actor_2.11-2.5.4.jar,SparkSenti-0.1/lib/akka-slf4j_2.11-2.5.4.jar,SparkSenti-0.1/lib/antlr4-runtime-4.6.jar,SparkSenti-0.1/lib/args4j-2.0.23.jar,SparkSenti-0.1/lib/async-http-client-1.7.16.jar,SparkSenti-0.1/lib/bioresources-1.1.24.jar,SparkSenti-0.1/lib/clearnlp-2.0.2.jar,SparkSenti-0.1/lib/common_2.11-0.0.7.jar,SparkSenti-0.1/lib/common-scala_2.10-1.1.2.jar,SparkSenti-0.1/lib/commons-codec-1.4.jar,SparkSenti-0.1/lib/commons-io-2.5.jar,SparkSenti-0.1/lib/commons-lang3-3.4.jar,SparkSenti-0.1/lib/config-1.3.1.jar,SparkSenti-0.1/lib/dispatch-core_2.10-0.11.0.jar,SparkSenti-0.1/lib/ejml-0.23.jar,SparkSenti-0.1/lib/guava-14.0.1.jar,SparkSenti-0.1/lib/hppc-0.5.2.jar,SparkSenti-0.1/lib/jackson-annotations-2.8.9.jar,SparkSenti-0.1/lib/jackson-core-2.8.9.jar,SparkSenti-0.1/lib/jackson-databind-2.8.9.jar,SparkSenti-0.1/lib/jackson-datatype-jdk8-2.8.9.jar,SparkSenti-0.1/lib/jackson-datatype-jsr310-2.8.9.jar,SparkSenti-0.1/lib/javax.json-api-1.0.jar,SparkSenti-0.1/lib/javax.servlet-2.5.0.v201103041518.jar,SparkSenti-0.1/lib/jaxb-api-2.2.7.jar,SparkSenti-0.1/lib/jetty-continuation-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-http-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-io-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-security-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-server-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-servlet-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-util-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-webapp-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-xml-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jline-2.12.1.jar,SparkSenti-0.1/lib/joda-time-2.9.9.jar,SparkSenti-0.1/lib/jollyday-0.4.7.jar,SparkSenti-0.1/lib/jregex-1.2_01.jar,SparkSenti-0.1/lib/json4s-ast_2.11-3.5.2.jar,SparkSenti-0.1/lib/json4s-core_2.11-3.5.2.jar,SparkSenti-0.1/lib/json4s-jackson_2.11-3.5.2.jar,SparkSenti-0.1/lib/json4s-scalap_2.11-3.5.2.jar,SparkSenti-0.1/lib/liblinear-1.94.jar,SparkSenti-0.1/lib/libsvm-3.17.jar,SparkSenti-0.1/lib/log4j-1.2.17.jar,SparkSenti-0.1/lib/logback-classic-1.0.10.jar,SparkSenti-0.1/lib/logback-core-1.0.10.jar,SparkSenti-0.1/lib/macro-compat_2.11-1.1.1.jar,SparkSenti-0.1/lib/maltparser-1.9.0.jar,SparkSenti-0.1/lib/morpha-stemmer-1.0.5.jar,SparkSenti-0.1/lib/netty-3.6.3.Final.jar,SparkSenti-0.1/lib/nlptools-core_2.10-2.4.5.jar,SparkSenti-0.1/lib/nlptools-stem-morpha_2.10-2.4.5.jar,SparkSenti-0.1/lib/paranamer-2.8.jar,SparkSenti-0.1/lib/play-functional_2.11-2.6.7.jar,SparkSenti-0.1/lib/play-json_2.11-2.6.7.jar,SparkSenti-0.1/lib/processors-corenlp_2.11-6.1.3.jar,SparkSenti-0.1/lib/processors-main_2.11-6.1.3.jar,SparkSenti-0.1/lib/processors-modelscorenlp_2.11-6.1.3.jar,SparkSenti-0.1/lib/processors-modelsmain_2.11-6.1.3.jar,SparkSenti-0.1/lib/processors-odin_2.11-6.1.3.jar,SparkSenti-0.1/lib/scala-java8-compat_2.11-0.7.0.jar,SparkSenti-0.1/lib/scala-logging_2.11-3.4.0.jar,SparkSenti-0.1/lib/scala-parser-combinators_2.11-1.0.3.jar,SparkSenti-0.1/lib/scala-reflect-2.11.8.jar,SparkSenti-0.1/lib/scala-xml_2.11-1.0.6.jar,SparkSenti-0.1/lib/scalaz-core_2.11-7.2.18.jar,SparkSenti-0.1/lib/scopt_2.10-2.1.0.jar,SparkSenti-0.1/lib/slf4j-api-1.7.10.jar,SparkSenti-0.1/lib/snakeyaml-1.14.jar,SparkSenti-0.1/lib/spark-corenlp-0.2.0-s_2.11.jar,SparkSenti-0.1/lib/sparksenti_2.11-0.1.jar,SparkSenti-0.1/lib/spark-sql_2.10-1.0.0.jar,SparkSenti-0.1/lib/stanford-corenlp-3.3.0.jar,SparkSenti-0.1/lib/stanford-corenlp-3.3.0-models.jar,SparkSenti-0.1/lib/stanford-corenlp-3.5.1.jar,SparkSenti-0.1/lib/stanford-corenlp-3.5.1-models.jar,SparkSenti-0.1/lib/stanford-parser-3.4.jar,SparkSenti-0.1/lib/unfiltered_2.10-0.7.0.jar,SparkSenti-0.1/lib/unfiltered-filter_2.10-0.7.0.jar,SparkSenti-0.1/lib/unfiltered-jetty_2.10-0.7.0.jar,SparkSenti-0.1/lib/unfiltered-util_2.10-0.7.0.jar,SparkSenti-0.1/lib/xalan-2.7.0.jar,SparkSenti-0.1/lib/xercesImpl-2.8.0.jar,SparkSenti-0.1/lib/xml-apis-1.3.03.jar,SparkSenti-0.1/lib/xom-1.2.10.jar\r\n",
      "\u001b]0;fregosi1@bright73:~\u0007[fregosi1@bright73 ~]$ spark-submit --master yarn --conf spark.executorEnv.JAVA_ \r",
      "HOME=/usr/lib/jvm/jre-1.8.0-openjdk/ --class algo1_worksheet.test --jars SparkSe \r",
      "nti-0.1/lib/akka-actor_2.11-2.5.4.jar,SparkSenti-0.1/lib/akka-slf4j_2.11-2.5.4.j \r",
      "ar,SparkSenti-0.1/lib/antlr4-runtime-4.6.jar,SparkSenti-0.1/lib/args4j-2.0.23.ja \r",
      "r,SparkSenti-0.1/lib/async-http-client-1.7.16.jar,SparkSenti-0.1/lib/bioresource \r",
      "s-1.1.24.jar,SparkSenti-0.1/lib/clearnlp-2.0.2.jar,SparkSenti-0.1/lib/common_2.1 \r",
      "1-0.0.7.jar,SparkSenti-0.1/lib/common-scala_2.10-1.1.2.jar,SparkSenti-0.1/lib/co \r",
      "mmons-codec-1.4.jar,SparkSenti-0.1/lib/commons-io-2.5.jar,SparkSenti-0.1/lib/com \r",
      "mons-lang3-3.4.jar,SparkSenti-0.1/lib/config-1.3.1.jar,SparkSenti-0.1/lib/dispat \r",
      "ch-core_2.10-0.11.0.jar,SparkSenti-0.1/lib/ejml-0.23.jar,SparkSenti-0.1/lib/guav \r",
      "a-14.0.1.jar,SparkSenti-0.1/lib/hppc-0.5.2.jar,SparkSenti-0.1/lib/jackson-annota \r",
      "tions-2.8.9.jar,SparkSenti-0.1/lib/jackson-core-2.8.9.jar,SparkSenti-0.1/lib/jac \r",
      "kson-databind-2.8.9.jar,SparkSenti-0.1/lib/jackson-datatype-jdk8-2.8.9.jar,Spark \r",
      "Senti-0.1/lib/jackson-datatype-jsr310-2.8.9.jar,SparkSenti-0.1/lib/javax.json-ap \r",
      "i-1.0.jar,SparkSenti-0.1/lib/javax.servlet-2.5.0.v201103041518.jar,SparkSenti-0. \r",
      "1/lib/jaxb-api-2.2.7.jar,SparkSenti-0.1/lib/jetty-continuation-7.6.9.v20130131.j \r",
      "ar,SparkSenti-0.1/lib/jetty-http-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-io \r",
      "-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-security-7.6.9.v20130131.jar,Spark \r",
      "Senti-0.1/lib/jetty-server-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-servlet- \r",
      "7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-util-7.6.9.v20130131.jar,SparkSenti \r",
      "-0.1/lib/jetty-webapp-7.6.9.v20130131.jar,SparkSenti-0.1/lib/jetty-xml-7.6.9.v20 \r",
      "130131.jar,SparkSenti-0.1/lib/jline-2.12.1.jar,SparkSenti-0.1/lib/joda-time-2.9. \r",
      "9.jar,SparkSenti-0.1/lib/jollyday-0.4.7.jar,SparkSenti-0.1/lib/jregex-1.2_01.jar \r",
      ",SparkSenti-0.1/lib/json4s-ast_2.11-3.5.2.jar,SparkSenti-0.1/lib/json4s-core_2.1 \r",
      "1-3.5.2.jar,SparkSenti-0.1/lib/json4s-jackson_2.11-3.5.2.jar,SparkSenti-0.1/lib/ \r",
      "json4s-scalap_2.11-3.5.2.jar,SparkSenti-0.1/lib/liblinear-1.94.jar,SparkSenti-0. \r",
      "1/lib/libsvm-3.17.jar,SparkSenti-0.1/lib/log4j-1.2.17.jar,SparkSenti-0.1/lib/log \r",
      "back-classic-1.0.10.jar,SparkSenti-0.1/lib/logback-core-1.0.10.jar,SparkSenti-0. \r",
      "1/lib/macro-compat_2.11-1.1.1.jar,SparkSenti-0.1/lib/maltparser-1.9.0.jar,SparkS \r",
      "enti-0.1/lib/morpha-stemmer-1.0.5.jar,SparkSenti-0.1/lib/netty-3.6.3.Final.jar,S \r",
      "parkSenti-0.1/lib/nlptools-core_2.10-2.4.5.jar,SparkSenti-0.1/lib/nlptools-stem- \r",
      "morpha_2.10-2.4.5.jar,SparkSenti-0.1/lib/paranamer-2.8.jar,SparkSenti-0.1/lib/pl \r",
      "ay-functional_2.11-2.6.7.jar,SparkSenti-0.1/lib/play-json_2.11-2.6.7.jar,SparkSe \r",
      "nti-0.1/lib/processors-corenlp_2.11-6.1.3.jar,SparkSenti-0.1/lib/processors-main \r",
      "_2.11-6.1.3.jar,SparkSenti-0.1/lib/processors-modelscorenlp_2.11-6.1.3.jar,Spark \r",
      "Senti-0.1/lib/processors-modelsmain_2.11-6.1.3.jar,SparkSenti-0.1/lib/processors \r",
      "-odin_2.11-6.1.3.jar,SparkSenti-0.1/lib/scala-java8-compat_2.11-0.7.0.jar,SparkS \r",
      "enti-0.1/lib/scala-logging_2.11-3.4.0.jar,SparkSenti-0.1/lib/scala-parser-combin \r",
      "ators_2.11-1.0.3.jar,SparkSenti-0.1/lib/scala-reflect-2.11.8.jar,SparkSenti-0.1/ \r",
      "lib/scala-xml_2.11-1.0.6.jar,SparkSenti-0.1/lib/scalaz-core_2.11-7.2.18.jar,Spar \r",
      "kSenti-0.1/lib/scopt_2.10-2.1.0.jar,SparkSenti-0.1/lib/slf4j-api-1.7.10.jar,Spar \r",
      "kSenti-0.1/lib/snakeyaml-1.14.jar,SparkSenti-0.1/lib/spark-corenlp-0.2.0-s_2.11. \r",
      "jar,SparkSenti-0.1/lib/sparksenti_2.11-0.1.jar,SparkSenti-0.1/lib/spark-sql_2.10 \r",
      "-1.0.0.jar,SparkSenti-0.1/lib/stanford-corenlp-3.3.0.jar,SparkSenti-0.1/lib/stan \r",
      "ford-corenlp-3.3.0-models.jar,SparkSenti-0.1/lib/stanford-corenlp-3.5.1.jar,Spar \r",
      "kSenti-0.1/lib/stanford-corenlp-3.5.1-models.jar,SparkSenti-0.1/lib/stanford-par \r",
      "ser-3.4.jar,SparkSenti-0.1/lib/unfiltered_2.10-0.7.0.jar,SparkSenti-0.1/lib/unfi \r",
      "ltered-filter_2.10-0.7.0.jar,SparkSenti-0.1/lib/unfiltered-jetty_2.10-0.7.0.jar, \r",
      "SparkSenti-0.1/lib/unfiltered-util_2.10-0.7.0.jar,SparkSenti-0.1/lib/xalan-2.7.0 \r",
      ".jar,SparkSenti-0.1/lib/xercesImpl-2.8.0.jar,SparkSenti-0.1/lib/xml-apis-1.3.03. \r",
      "jar,SparkSenti-0.1/lib/xom-1.2.10.jar SparkSenti-0.1/lib/sparksenti_2.11-0.1.jar \r",
      " input_tweet_1521125436.670453.json > defaultoutput.txt\r\n",
      "18/03/15 16:05:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "18/03/15 16:05:32 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\r\n",
      "\r",
      "[Stage 0:>                                                          (0 + 2) / 2]18/03/15 16:05:59 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, bright73.cm.cluster): com.fasterxml.jackson.databind.JsonMappingException: No content to map due to end-of-input\r\n",
      " at [Source: ; line: 1, column: 1]\r\n",
      "\tat com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148)\r\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:3781)\r\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:3694)\r\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2005)\r\n",
      "\tat play.api.libs.json.jackson.JacksonJson$.parseJsValue(JacksonJson.scala:235)\r\n",
      "\tat play.api.libs.json.StaticBinding$.parseJsValue(StaticBinding.scala:16)\r\n",
      "\tat play.api.libs.json.Json$.parse(Json.scala:171)\r\n",
      "\tat algo1_worksheet.test$$anonfun$1.apply(algo1.scala:70)\r\n",
      "\tat algo1_worksheet.test$$anonfun$1.apply(algo1.scala:70)\r\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n",
      "\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\r\n",
      "\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\r\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1070)\r\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1070)\r\n",
      "\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1933)\r\n",
      "\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1933)\r\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n",
      "\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\r\n",
      "18/03/15 16:05:59 ERROR TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job\r\n",
      "Exception in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 4, bright73.cm.cluster): com.fasterxml.jackson.databind.JsonMappingException: No content to map due to end-of-input\r\n",
      " at [Source: ; line: 1, column: 1]\r\n",
      "\tat com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148)\r\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:3781)\r\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:3694)\r\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2005)\r\n",
      "\tat play.api.libs.json.jackson.JacksonJson$.parseJsValue(JacksonJson.scala:235)\r\n",
      "\tat play.api.libs.json.StaticBinding$.parseJsValue(StaticBinding.scala:16)\r\n",
      "\tat play.api.libs.json.Json$.parse(Json.scala:171)\r\n",
      "\tat algo1_worksheet.test$$anonfun$1.apply(algo1.scala:70)\r\n",
      "\tat algo1_worksheet.test$$anonfun$1.apply(algo1.scala:70)\r\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n",
      "\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\r\n",
      "\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\r\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1070)\r\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1070)\r\n",
      "\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1933)\r\n",
      "\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1933)\r\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n",
      "\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\r\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\r\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n",
      "\tat scala.Option.foreach(Option.scala:257)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\r\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1072)\r\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\r\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1065)\r\n",
      "\tat algo1_worksheet.test$.analysis2(algo1.scala:92)\r\n",
      "\tat algo1_worksheet.test$.delayedEndpoint$algo1_worksheet$test$1(algo1.scala:75)\r\n",
      "\tat algo1_worksheet.test$delayedInit$body.apply(algo1.scala:64)\r\n",
      "\tat scala.Function0$class.apply$mcV$sp(Function0.scala:34)\r\n",
      "\tat scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)\r\n",
      "\tat scala.App$$anonfun$main$1.apply(App.scala:76)\r\n",
      "\tat scala.App$$anonfun$main$1.apply(App.scala:76)\r\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n",
      "\tat scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)\r\n",
      "\tat scala.App$class.main(App.scala:76)\r\n",
      "\tat algo1_worksheet.test$.main(algo1.scala:64)\r\n",
      "\tat algo1_worksheet.test.main(algo1.scala)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n",
      "Caused by: com.fasterxml.jackson.databind.JsonMappingException: No content to map due to end-of-input\r\n",
      " at [Source: UNKNOWN; line: 1, column: 1]\r\n",
      "\tat com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148)\r\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:3781)\r\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:3694)\r\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2005)\r\n",
      "\tat play.api.libs.json.jackson.JacksonJson$.parseJsValue(JacksonJson.scala:235)\r\n",
      "\tat play.api.libs.json.StaticBinding$.parseJsValue(StaticBinding.scala:16)\r\n",
      "\tat play.api.libs.json.Json$.parse(Json.scala:171)\r\n",
      "\tat algo1_worksheet.test$$anonfun$1.apply(algo1.scala:70)\r\n",
      "\tat algo1_worksheet.test$$anonfun$1.apply(algo1.scala:70)\r\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n",
      "\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\r\n",
      "\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\r\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1070)\r\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1070)\r\n",
      "\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1933)\r\n",
      "\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1933)\r\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n",
      "\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\u001b]0;fregosi1@bright73:~\u0007[fregosi1@bright73 ~]$ \n",
      "\u001b[0m[\u001b[0m\u001b[32msuccess\u001b[0m] \u001b[0m\u001b[0mTotal time: 32 s, completed 15 mars 2018 16:06:01\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd Algo1\n",
    "rm -rf $TWEET_JSON_FILENAME\n",
    "sbt \"submit $TWEET_JSON_FILENAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération du resultat de l'analyse : fichier text (id_tweet sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from idea.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading global plugins from /Users/Frego/.sbt/1.0/plugins\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from plugins.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading project definition from /Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/project\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mLoading settings from build.sbt ...\u001b[0m\n",
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mSet current project to SparkSenti (in build file:/Users/Frego/Documents/Centrale/4A/OPTION/08_Projet/SparkSenti/Algo1/)\u001b[0m\n",
      "spawn scp fregosi1@156.18.90.100:'defaultoutput.txt' .\r\n",
      "fregosi1@156.18.90.100's password: \r\n",
      "\r",
      "defaultoutput.txt                             100%    0     0.0KB/s   00:00    \r\n",
      "\n",
      "\u001b[0m[\u001b[0m\u001b[32msuccess\u001b[0m] \u001b[0m\u001b[0mTotal time: 0 s, completed 15 mars 2018 15:55:19\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd Algo1\n",
    "sbt \"getOutput 'defaultoutput.txt'\"\n",
    "cp defaultoutput.txt ../tweets-database/$OUTPUT_TWEET_TXT_FILENAME\n",
    "rm -rf defaultoutput.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour test de viz (evite de lancer le workflow complet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OUTPUT_TWEET_TXT_FILENAME=output_test.txt\n",
      "env: TWEET_JSON_FILENAME=input_test.json\n"
     ]
    }
   ],
   "source": [
    "%env OUTPUT_TWEET_TXT_FILENAME=output_test.txt\n",
    "%env TWEET_JSON_FILENAME=input_test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping des données pour Viz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "file_output=open(\"tweets-database/\"+os.environ['OUTPUT_TWEET_TXT_FILENAME'],\"r\", encoding=\"utf8\");\n",
    "file_tweet=open(\"tweets-database/\"+os.environ['TWEET_JSON_FILENAME'],\"r\", encoding=\"utf8\");\n",
    "\n",
    "\n",
    "def parserByTweet(file_output,file_tweet):\n",
    "    fileText = file_output.read().replace(\"Négatif\",\"Negatif\")\n",
    "    fileTweets = fileText.split(\"\\n\\n\")[1:]\n",
    "    \n",
    "    tweetList = []\n",
    "    \n",
    "    j=0\n",
    "    for tweet in fileTweets[:-1]:\n",
    "        j=j+1\n",
    "        print(j)\n",
    "        data=tweet.split(\"\\n\")\n",
    "        for i in range(len(data)-1):\n",
    "            if data[i]==\"Score: Positif\" or data[i]==\"Score: Negatif\" or data[i]==\"Score: Neutre\":\n",
    "                result=data[i][7:]\n",
    "                tweet_id=data[i+1][9:]\n",
    "        \n",
    "        for tweet_json in json_lines.reader(file_tweet):\n",
    "            if int(tweet_id)==int(tweet_json[\"id\"]):\n",
    "                new={}\n",
    "                new[\"result\"]=result\n",
    "                new[\"tweet\"]=tweet_json\n",
    "                tweetList.append(new)  \n",
    "                break\n",
    "    return(tweetList)\n",
    "    \n",
    "data_to_viz=parserByTweet(file_output,file_tweet)\n",
    "print(len(data_to_viz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.2)",
   "language": "python",
   "name": "pythonwithpixiedustspark22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
